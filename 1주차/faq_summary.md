# faq summary

---

### 1. 인공지능에서 지능에 해당하는 기능

- **학습(Learning)**: 데이터를 통해 패턴과 관계를 인식하고, 이를 기반으로 새로운 정보를 이해하는 능력입니다. 인공지능은 다양한 머신러닝 알고리즘을 통해 데이터를 학습하고 예측하는 모델을 개발합니다. 예를 들어, 이미지 인식 AI는 수많은 이미지 데이터를 학습해 특정 대상이 포함된 새로운 이미지를 인식합니다.
- **추론(Reasoning)**: 학습한 정보를 바탕으로 논리적으로 결론을 도출하는 능력입니다. 예를 들어, 체스 인공지능은 현재 체스판의 상황을 분석해 가능한 수를 추론하고, 최적의 전략을 결정합니다.
- **문제 해결(Problem Solving)**: 주어진 문제를 분석하고 최적의 해결책을 찾는 능력으로, 복잡한 변수와 제약 조건을 고려해 해결책을 도출합니다. 물류 시스템의 경로 최적화 알고리즘이 대표적인 예입니다.
- **이해(Understanding)**: 자연어 처리(NLP) 기술을 통해 인간의 언어와 감정을 이해하는 능력입니다. 텍스트 분석, 감정 분석, 언어 번역 등에 적용되며, 사용자의 질문을 이해하고 적절한 답변을 제공합니다.

---

### 2. 인공지능의 종류

- **지도학습(Supervised Learning)**: 정답(레이블)이 있는 데이터를 학습하여 패턴을 인식하는 방법입니다. 모델이 학습한 패턴을 새로운 데이터에 적용해 예측할 수 있으며, 스팸 이메일 분류와 같은 분야에 사용됩니다. 대표 알고리즘에는 선형 회귀, 로지스틱 회귀 등이 있습니다.
- **비지도학습(Unsupervised Learning)**: 레이블이 없는 데이터를 분석하여 데이터의 구조와 관계를 이해합니다. 주로 데이터를 클러스터링하거나 차원을 축소해 패턴을 파악합니다. 고객 세분화와 같은 분석에서 자주 사용됩니다.
- **반지도학습(Semisupervised Learning)**: 일부 데이터에만 레이블이 있는 상태에서 학습합니다. 데이터가 많으나 레이블이 적은 경우에 효과적이며, 데이터의 구조를 활용해 레이블이 없는 데이터의 특성을 파악할 수 있습니다.
- **강화학습(Reinforcement Learning)**: 에이전트가 환경과 상호작용하면서 보상을 최대화하는 행동을 학습하는 방법입니다. 에이전트가 특정 행동을 수행한 후 보상 또는 페널티를 받으며 최적의 행동을 학습합니다. 게임 AI, 로봇 제어 등에서 활용됩니다.

---

### 3. 전통적 프로그래밍과 인공지능 프로그램의 차이

- **전통적 프로그래밍**: 프로그래머가 명확한 규칙과 로직을 작성합니다. 주어진 입력에 따라 미리 정해진 결과를 출력하며, 예측 불가능한 상황에 대한 대응은 제한적입니다.
- **인공지능 프로그램**: 데이터를 통해 패턴과 규칙을 학습한 모델이 예측과 추론을 수행합니다. 명확한 규칙 없이도 데이터에 기반하여 유연하게 동작할 수 있어, 새로운 데이터에도 적응할 수 있습니다.

---

### 4. 머신러닝과 딥러닝의 차이

- **머신러닝(Machine Learning)**: 데이터를 통해 학습 알고리즘을 개발하는 분야로, 특징 추출과 예측 모델을 통해 데이터 간의 관계를 파악합니다. 단순한 회귀 모델부터 복잡한 의사결정 트리까지 다양한 알고리즘이 사용됩니다.
- **딥러닝(Deep Learning)**: 다층 신경망을 사용해 복잡한 패턴을 학습하는 머신러닝의 하위 분야입니다. 이미지 인식, 음성 인식, 자연어 처리 등 복잡한 문제 해결에 적합하며, CNN과 RNN이 대표적인 딥러닝 모델입니다.

---

### 5. Classification과 Regression의 차이

- **Classification**: 데이터를 특정 범주로 분류하는 작업입니다. 예를 들어, 이메일을 스팸과 비스팸으로 분류하거나 이미지 속 물체를 특정 카테고리로 구분하는 작업이 여기에 해당됩니다.
- **Regression**: 데이터를 기반으로 연속적인 값을 예측합니다. 예를 들어, 주택의 면적과 위치를 기반으로 주택 가격을 예측하는 작업입니다.

---

### 6. 차원의 저주(Curse of Dimensionality)

고차원 데이터에서 발생하는 문제로, 차원이 증가할수록 데이터 공간의 부피가 기하급수적으로 커져 데이터가 희소해지고, 모델의 학습이 어려워집니다. 차원의 저주로 인해 모델이 과적합되기 쉽습니다. 이를 해결하기 위해 주성분 분석(PCA)과 같은 차원 축소 기법을 사용합니다.

---

### 7. 차원 축소의 필요성

- **모델 복잡성 감소**: 차원을 줄여 모델이 불필요한 특징에 과적합되지 않도록 방지합니다.
- **계산 효율성 향상**: 차원을 줄이면 모델의 학습 시간과 예측 시간을 단축할 수 있습니다.
- **데이터 시각화 용이**: 고차원 데이터를 2D 또는 3D로 변환해 시각화하면 데이터의 패턴과 구조를 직관적으로 이해하기 쉽습니다.

---

### 8. Ridge와 Lasso

- **공통점**: 두 방법 모두 규제(Regularization)를 통해 모델의 복잡도를 줄여 과적합을 방지합니다.
- **Ridge 회귀**: L2 정규화를 사용해 모든 특성의 가중치를 작게 만들어 모델에 기여하게 합니다.
- **Lasso 회귀**: L1 정규화를 사용해 불필요한 특성의 가중치를 0으로 만들어 특성 선택의 효과를 제공합니다.

---

### 9. Overfitting vs. Underfitting

- **Overfitting(과적합)**: 모델이 훈련 데이터에 과도하게 맞춰져 새로운 데이터에 대해 성능이 떨어지는 현상입니다. 복잡한 모델, 부족한 데이터, 과도한 학습이 원인이며, 이를 해결하기 위해 규제 기법이나 교차 검증을 사용합니다.
- **Underfitting(과소적합)**: 모델이 훈련 데이터의 패턴을 제대로 학습하지 못하는 현상입니다. 모델이 지나치게 단순하거나 학습 시간이 부족할 때 발생하며, 이를 방지하려면 복잡한 모델을 사용하고 충분히 학습해야 합니다.

---

### 10. Feature Engineering과 Feature Selection

- **Feature Engineering**: 모델이 데이터를 더 잘 이해할 수 있도록 변환하는 과정입니다. 예를 들어, 날짜 데이터를 년, 월, 일로 분리하거나 텍스트 데이터를 벡터화하는 작업입니다.
- **Feature Selection**: 모델에 중요한 특징만을 남기고 불필요한 특징을 제거하는 과정으로, 과적합을 방지하고 학습 시간을 단축할 수 있습니다.

---

### 11. 전처리의 목적과 방법

- **노이즈 제거**: 데이터에서 불필요한 요소를 제거합니다.
- **이상치 처리**: 비정상적 데이터를 제거 또는 대체하여 모델의 성능을 개선합니다.
- **결측치 처리**: 평균값이나 중앙값으로 결측치를 대체하거나, 결측치가 있는 행을 제거하여 학습 오류를 방지합니다.
- **정규화 및 표준화**: 모델이 효율적으로 학습하도록 값을 특정 범위로 조정합니다.

---

### 12. EDA(탐색적 데이터 분석)

EDA는 데이터의 분포, 상관관계, 이상치 등을 시각화와 통계적 요약을 통해 분석하여, 데이터의 특성을 파악하는 과정입니다.

---

### 13. 회귀에서 절편과 기울기의 의미

- **절편(Intercept)**: 독립 변수가 0일 때의 종속 변수 값으로, 회귀선이 y축과 만나는 점입니다.
- **기울기(Slope)**: 독립 변수가 변화할 때 종속 변수가 얼마나 변하는지를 나타내며, 두 변수의 관계 강도를 보여줍니다.

---

### 14. Activation Function 함수의 필요성과 Softmax, Sigmoid 함수의 차이

활성화 함수는 뉴런이 출력 신호를 전달할지를 결정하는 비선형 함수입니다. Sigmoid는 이진 분류 문제에, Softmax는 다중 클래스 분류에 사용됩니다.

---

### 15. 순전파(Forward Propagation)와 역전파(Backward Propagation)

- **순전파**: 입력을 통해 출력 값을 계산.
- **역전파**: 출력과 실제 값 간의 오차를 계산하여 가중치를 조정.

---

### 16. 손실 함수란?

모델의 예측 성능을 평가하는 함수로, 실제 값과 예측 값의 차이를 측정합니다.

---

### 17. 옵티마이저

손실 함수를 최소화하도록 가중치를 업데이트하여 최

적화하는 알고리즘입니다.

---

### 18. 경사하강법

손실 함수를 최소화하기 위해 가중치를 반복적으로 조정하여 최적의 모델을 찾는 방법입니다.

---

### 19. 교차검증과 K-fold 교차검증

- **교차검증**: 모델이 과적합되지 않도록 데이터를 학습/검증 세트로 나누어 평가합니다.
- **K-fold 교차검증**: 데이터를 K개의 세트로 나누어 각 세트로 학습/검증을 반복하여 평균 성능을 측정합니다.

---

### 20. 하이퍼파라미터 튜닝

모델의 성능을 최적화하기 위해 하이퍼파라미터를 조정하는 과정으로, 그리드 서치와 랜덤 서치 방법이 있습니다.

---

### 21. CNN의 합성곱(Convolution)의 역할

입력 이미지의 특정 특징(모서리, 색상 등)을 추출하여, 모델이 이미지의 중요한 패턴을 이해할 수 있도록 합니다.

---

### 22. CNN의 풀링층(Pooling Layer)의 역할

입력 데이터의 크기를 줄여 계산 비용을 줄이고, 중요한 특징을 요약하여 모델 성능을 향상합니다.

---

### 23. CNN의 Dense Layer

완전 연결 층으로, 이전 층에서 추출된 특징을 기반으로 최종 예측을 수행합니다.

---

### 24. CNN의 Stride와 Filter

- **Stride**: 필터가 이동하는 간격으로, 출력 크기에 영향을 미칩니다.
- **Filter**: 특정 특징을 추출하는 매개변수로, 학습 과정에서 자동으로 조정됩니다.

---

### 25. RNN의 사용 이유와 한계

순차적 데이터 처리와 시간 종속성 학습에 적합하지만, 기울기 소실 문제로 장기 종속성 학습이 어렵습니다.

---

### 26. LSTM의 사용 이유와 한계

장기 종속성을 학습할 수 있지만, 복잡한 구조로 인해 계산 비용이 높습니다.

---

### 27. GRU의 사용 이유와 차별성

LSTM과 유사한 성능을 유지하며, 구조가 단순하여 계산 비용이 낮습니다.

---

### 28. 결정 트리의 불순도(Impurity)와 지니 계수(Gini Index)

- **불순도**: 노드 내 데이터의 혼합 정도로, 지니 계수는 노드가 순수할수록 0에 가까운 값을 가집니다.

---

### 29. 앙상블(Ensemble)

여러 모델을 결합하여 예측 성능을 향상시키는 방법으로, 배깅과 부스팅이 주요 기법입니다.

---

### 30. 부트 스트랩핑(Bootstrapping)

중복 허용 샘플링 기법으로, 다양한 샘플에서 모델의 성능을 평가합니다.

---

### 31. 배깅(Bagging)

여러 모델의 예측을 평균내거나 다수결로 결합하여 성능을 향상시키는 방법입니다.

---

### 32. 주성분 분석(PCA)

고차원 데이터를 주요 정보로 요약해 저차원으로 변환하는 기법으로, 모델 학습 시간을 단축하고 계산 효율성을 높입니다.

---

### 33. Dense Layer(완전 연결 층)

CNN에서 추출된 특징을 결합해 복잡한 패턴을 학습하고 최종 예측을 수행하는 층입니다.

---