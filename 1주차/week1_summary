ot
- https://github.com/MyungKyuYi/AI-class/blob/main/README.md
- 중간고사, 기말고사에 출제됨
인공지능 기초를 위한 FAQ

1. 인공지능에서 지능에 해당하는 기능은 무엇인가?

2. 인공지능의 종류 3가지에 대해서 설명하시오 (지도학습, 반지도학습, 강화학습)

3. 전통적인 프로그래밍 방법과 인공지능 프로그램의 차이점은 무엇인가?

4. 딥러닝과 머신러닝의 차이점은 무엇인가?

5. Classification과 Regression의 주된 차이점은?

6. 머신러닝에서 차원의 저주(curse of dimensionality)란?

7. Dimensionality Reduction는 왜 필요한가?

8. Ridge와 Lasso의 공통점과 차이점? (Regularization, 규제 , Scaling)

9. Overfitting vs. Underfitting

10. Feature Engineering과 Feature Selection의 차이점은?

11. 전처리(Preprocessing)의 목적과 방법? (노이즈, 이상치, 결측치)

12. EDA(Explorary Data Analysis)란? 데이터의 특성 파악(분포, 상관관계)

13. 회귀에서 절편과 기울기가 의미하는 바는? 딥러닝과 어떻게 연관되는가?

14. Activation function 함수를 사용하는 이유? Softmax, Sigmoid 함수의 차이는?

15. Forward propagation, Backward propagation이란?

16. 손실함수란 무엇인가? 가장 많이 사용하는 손실함수 4가지 종류는?

17. 옵티마이저(optimizer)란 무엇일까? 옵티마이저와 손실함수의 차이점은?

18. 경사하강법 의미는? (확률적 경사하강법, 배치 경사하강법, 미치 배치경사하강법)

19. 교차검증, K-fold 교차검증의 의미와 차이

20. 하이퍼파라미터 튜닝이란 무엇인가?

21. CNN의 합성곱의 역활은?

22. CNN의 풀링층의 역활은?

23. CNN의 Dense Layer의 역활은?

24. CNN의 stride, filter의 역활? 필터의 가중치는 어떻게 결정되는가?

25. RNN을 사용하는 이유와 한계점은?

26. LSTM을 사용하는 이유와 한계점은?

27. GRU을 사용하는 이유와 차별성은?

28. 결정트리에서  불순도(Impurity) – 지니 계수(Gini Index)란 무엇인가?

29. 앙상블이란 무엇인가?

30. 부트 스트랩핑(bootstraping)이란 무엇인가?

31. 배깅(Bagging)이란 무엇인가?

32. 주성분 분석(PCA) 이란 무엇인가?

33. Dense Layer란 무엇인가?

week1 

### 인공지능에서 지능에 해당하는 기능은 무엇인가?
- **분류, 회귀, 인식** 등을 통해 기계가 데이터를 처리하고 예측하는 기능을 수행함.

### 인공지능의 종류 3가지에 대해서 설명하시오 (지도학습, 비지도학습, 강화학습)
- **지도학습 (Supervised Learning)**: 정답(label)이 존재하여 데이터에 맞는 예측을 학습.
    - **분류** (Classification)
    - **회귀** (Regression)
- **비지도학습 (Unsupervised Learning)**: 정답(label)이 없이 데이터의 패턴을 찾음 (예: 군집화).
    - 최근 주목받고 있는 이유는 현실 데이터에 정답을 매기기 어렵기 때문.
- **강화학습 (Reinforcement Learning)**: 보상을 통해 올바른 행동을 학습하도록 훈련.
    - 정책(policy) 설정 후, 보상을 최대화하도록 학습함.
- **추가**: 반지도학습 (일부 데이터에만 정답을 제공하여 성능 개선), 
  자기 지도학습 (label 없이 지도학습 가능)

### 전통적인 프로그래밍 방법과 인공지능 프로그램의 차이점은 무엇인가?
- **전통적인 프로그래밍**: 규칙(rule)을 프로그래머가 직접 정의하여 프로그램에 입력함.
- **AI 프로그래밍**: 데이터를 통해 스스로 규칙을 학습하여 예측함.

### 딥러닝과 머신러닝의 차이점은?
- **딥러닝**: 원본 데이터를 입력하고 **특징 추출과 학습을 동시에** 수행함.
- **머신러닝**: 주요 특징을 선별하여 입력하고 학습함.

### Classification과 Regression의 주된 차이점은?
- **Classification**: 이산적 정보(범주형 변수) 예측.
- **Regression**: 연속적 정보(연속형 변수) 예측.
- **추가**: 분류 문제를 더 세밀히 나누면 회귀처럼 사용할 수 있음.

### 머신러닝에서 차원의 저주(Curse of Dimensionality)란? Dimensionality Reduction는 왜 필요한가?
- **차원의 저주**: 차원이 많아지면 데이터 분석이 어려워지며, 과적합 위험이 있음.
- **해결법**: 주요 특징만 선택하거나 차원 축소 기법(PCA 등) 사용.

### Ridge와 Lasso의 공통점과 차이점?
- **공통점**: 과적합 방지를 위한 규제(regularization) 방법.
- **차이점**:
    - Ridge: L2 규제 (제곱항)
    - Lasso: L1 규제 (절대값 항)

### Overfitting vs. Underfitting
- **Overfitting**: 학습 데이터에 과적합, 새로운 데이터 예측 실패 가능성 높음.
    - 해결법: 모델 단순화, 규제 적용.
- **Underfitting**: 학습 데이터조차 잘 예측하지 못하는 상태.
    - 해결법: 데이터 추가, 모델 복잡성 증가.

### 딥러닝 필수 요소
1. **손실 함수 (Loss Function)**: 예측값과 실제값 사이의 오차 계산.
2. **최적화 알고리즘 (Optimizer)**: 오차를 최소화하는 가중치, 바이어스 찾기.
3. **활성화 함수 (Activation Function)**: 비선형적 특성 추가로 정보 학습에 기여.
4. **역전파 (Backpropagation)**: 오차 역전파를 통해 가중치, 바이어스 수정.
5. **전진 전파 (Forward Propagation)**: 입력에서 출력을 향해 신호를 전달.
6. **One-Hot Encoding**: 범주형 변수 간 거리감을 없애기 위한 인코딩 방식.

### 손실 함수
- 회귀: MSE, MAE, RMSE 등.
- 분류:
    - 이진 분류: binary_crossentropy
    - 다중 분류: one-hot이면 categorical_crossentropy, 아니면 sparse_categorical_crossentropy

### 회귀에서 절편과 기울기가 의미하는 바는? 딥러닝과 어떻게 연관되는가?
- 절편과 기울기는 딥러닝에서 각 뉴런의 **가중치와 바이어스**에 해당함.
- **SGD**(확률적 경사 하강법)를 사용해 반복 학습으로 최적의 가중치, 바이어스 찾음.
- **절편**과 **기울기**는 회귀에서 직선을 정의하는 파라미터

### 딥러닝과 인공지능의 차이점
- **인공지능**: 여러 알고리즘을 포함하는 개념.
- **딥러닝**: 데이터의 규칙을 발견해 결과 예측하는 과정이며, 입력과 출력 간의 관계(f(x))를 학습.

### Noise
- 학습 데이터를 압축해 패턴을 찾고, 이를 통해 새로운 이미지 생성 등에 활용 (예: 생성적 AI).

### Self Fine-Tuning
- **자체 학습 튜닝**으로 성능을 향상시키며, 사용자 피드백을 통한 미세 조정보다 높은 효율.

### Mask
- 데이터의 일부를 가려서 복원하는 학습 방식.
    - 예: **BERT**가 단어를 가려 맞추는 방식으로 훈련.

### 데이터 퀄리티 3대 요소: 노이즈, 이상치, 결측치
- **Noise**: 함수 f(x)에서 벗어나는 데이터.
- **Outlier**: 데이터 범위를 벗어난 값.
- **Null**: 데이터 결측치.

### Cross-Validation
- 특정 데이터셋에 편향되지 않도록 **여러 검증 세트로 학습**하여 성능 평가.
- 교차 검증 후 모델 평가 평균을 내면 성능 측정이 더 정확해짐.

### Hyperparameter (하이퍼파라미터)
- 모델의 학습을 제어하는 사용자 조정 가능 변수 (예: 학습률, 배치 크기, 에포크 수 등).

### **Ridge와 Lasso의 공통점과 차이점?**
- **공통점** : 둘 다 모델의 복잡도를 줄여 과적합을 방지하는 **정규화(regularization)** 기법이다.
- **차이점** :
    - **Ridge (L2 Regularization)** : 모델의 가중치의 제곱값에 패널티를 부여한다.
    - **Lasso (L1 Regularization)** : 모델의 가중치의 절댓값에 패널티를 부여한다. 
      Lasso는 불필요한 특성의 가중치를 0으로 만들어 특성 선택의 효과를 가질 수 있다.
