WEEK4

### 1. CNN(Convolutional Layer)와 **Dense Layer**의 차이
- **CNN**: 이미지나 시계열 데이터의 공간적 특징을 추출하는 필터 기반 레이어. 파라미터 수가 적고 국소적 정보를 학습.
- **Dense Layer**: 모든 입력과 출력이 연결된 완전 연결 레이어로, 주로 최종 예측에 사용. 파라미터 수가 많고, 1D 벡터 형태로 데이터를 처리
- 차이점
    - **처리하는 데이터**: CNN은 2D/3D 데이터(예: 이미지)를 처리하고, Dense Layer는 1D 데이터(벡터)를 처리합니다.
    - **학습 방식**: CNN은 필터를 사용해 데이터의 국소적 패턴을 추출하고, Dense Layer는 모든 노드가 완전히 연결되어 정보를 학습합니다.
    - **파라미터 수**: CNN은 파라미터가 적고, Dense Layer는 파라미터가 많습니다.
- CNN은 주로 특징 추출에, Dense Layer는 최종 분류나 회귀 등의 예측에 사용된다고 볼 수 있음

---
### 2. 레이어가 많아질 때의 역전파 문제
- **역전파(backpropagation)란?**
    - 모델이 예측값과 실제값의 **오차**를 기반으로 각 레이어의 가중치를 업데이트하는 과정임.
    - 레이어가 많아질수록 최종 오차가 초기 레이어까지 전달되는 경로가 길어져 역전파가 어려워질 수 있음.
- **기울기 소실(vanishing gradient)**
    - 역전파 중에 기울기가 계속 작아지면서 초기 레이어로 갈수록 거의 0에 가까워짐.
    - 이로 인해 초기 레이어가 충분히 학습되지 않아 학습이 느려지거나 멈출 수 있음.
- **기울기 폭발(exploding gradient)**
    - 기울기가 지나치게 커지면 가중치가 큰 폭으로 업데이트되어 학습이 불안정해지고 발산할 수 있음.
- **해결 방법**
    - **잔차 연결(residual connections)**, **정규화 기법(batch normalization)**, 
      **LSTM** 등의 구조를 사용하면 깊은 네트워크에서도 기울기 소실이나 폭발 문제를 완화하여 안정적인 학습이 가능해짐.

### 3. 기울기 소실(Vanishing Gradient)
- **기울기 소실이란?**
    - 역전파(backpropagation) 과정에서, 네트워크의 깊이가 깊어질수록 기울기(gradient)가 점차 작아져 초기 레이어로 전파될 때 거의 0에 가까워지는 현상.
    - 이로 인해, 초기 레이어의 가중치가 거의 업데이트되지 않아 학습이 제대로 이루어지지 않음.
- **왜 발생하는가?**
    - 활성화 함수(예: Sigmoid, Tanh 등)가 너무 작은 값을 출력하면, 그 도함수(기울기)가 매우 작아짐.
    - 역전파 중에 이 작은 기울기가 계속 곱해지면서, 네트워크의 깊이가 깊어질수록 기울기가 기하급수적으로 작아짐.
- **기울기 소실의 결과**
    - 깊은 신경망에서, 특히 초기 레이어들은 오차 신호를 제대로 받지 못해 학습이 제대로 진행되지 않음.
    - 모델의 학습 속도가 느려지거나, 학습이 아예 멈추는 현상이 발생할 수 있음.
- **해결 방법**
    1. **ReLU(렐루) 함수 사용**: ReLU는 음수에 대해서 0을 출력하지만, 양수에 대해서는 기울기가 1이므로 기울기 소실 문제를 완화할 수 있음.
    2. **He Initialization**: 가중치를 적절하게 초기화하는 방법으로, 깊은 네트워크에서도 기울기 소실을 줄이는 데 도움을 줄 수 있음.
    3. **잔차 연결(residual connections)**: ResNet과 같은 구조에서 사용하는 기법으로, 기울기가 네트워크를 통과할 때 손실되지 않고 더 효과적으로 전달될 수 있도록 함.
    4. **배치 정규화(batch normalization)**: 각 레이어의 출력 값을 정규화하여 학습을 안정화하고 기울기 소실을 방지하는 데 도움을 줄 수 있음.
- 기울기 소실은 깊은 신경망의 학습을 어렵게 만드는 중요한 문제지만, 위의 기법들을 사용하면 이를 해결하고 보다 효율적인 학습을 할 수 있음.

### 4. Creeping
- **Creeping**은 일반적으로 네트워크가 점차적으로 잘못된 경로로 진행하는 현상이나 오류가 확산되는 상황을 묘사하는 용어일 수 있습니다. 
  딥러닝에서 **Creeping**은 종종 기울기 소실(Vanishing Gradient)이나 기울기 폭발(Exploding Gradient)처럼 학습 과정에서 문제가 서서히 확산되거나 악화되는 현상을 
  설명할 때 사용될 수 있습니다. 이런 현상은 네트워크가 깊어질수록 점차적으로 나타날 수 있습니다.

---
### 5. 배치 정규화 (Batch Normalization)
- *배치 정규화(Batch Normalization)**는 딥러닝 모델 학습 시 **내부 공변량 변화(internal covariate shift)**를 줄이기 위한 기법입니다. \
  이 기법은 각 레이어의 출력을 정규화하여 학습을 더 빠르고 안정적으로 만들고, 기울기 소실 문제를 완화하는 데 도움을 줍니다.
- **배치 정규화 과정**
    1. 입력 값의 평균과 분산을 구함.
    2. 평균과 분산을 이용해 입력 데이터를 정규화.
    3. 학습 가능한 파라미터인 **스케일(Scale)**과 **이동(Shift)**을 추가하여 정규화된 값을 조정.
- **배치 정규화의 장점**
    - 모델의 학습 속도를 가속화하고 안정성을 높임.
    - 과적합(overfitting)을 줄이는 데도 도움이 될 수 있음.
    - 기울기 소실 문제를 완화함.

---
### 6. 도메인 어드랩션 (Domain Adaptation)
- *도메인 어드랩션(Domain Adaptation)**은 **특정 도메인에서 학습된 모델을 다른 도메인으로 전이**하는 기법입니다.
  예를 들어, 한 도메인에서 훈련된 모델을 다른 도메인에 적용할 때 성능 저하가 발생할 수 있습니다. 
  이를 해결하기 위해 도메인 어드랩션은 **원본 도메인과 목표 도메인의 차이를 줄이기 위해 모델을 적응시키는** 과정입니다.
    - **예시**: 이미지를 학습한 모델을 비디오에 적용할 때, 이미지와 비디오는 다르기 때문에 모델의 성능이 저하될 수 있습니다.
      이때 도메인 어드랩션 기법을 사용하여 비디오 도메인에 맞게 모델을 조정합니다.

---
### 7. 트랜스퍼 러닝 (Transfer Learning)
- ***트랜스퍼 러닝(Transfer Learning)**은 **사전 훈련된 모델(pre-trained model)**을 사용하여 새로운 도메인이나 작업에서 학습을 빠르게 진행하는 방법**입니다. 
  이는 기본적으로 **기존에 훈련된 모델의 지식을 새로운 문제에 적용**하는 기법입니다. 
  트랜스퍼 러닝은 **셀프 슈퍼바이저 러닝(self-supervised learning)**과도 관련이 있습니다.
    - **셀프 슈퍼바이저 러닝**은 레이블이 없는 데이터를 사용하여 모델을 훈련하는 방법입니다.
    - **사전 훈련된 모델**을 사용하면, 대규모 데이터셋에서 훈련된 모델을 기반으로 빠르게 성능을 향상시킬 수 있습니다. 
      예를 들어, 이미지 인식에서는 ImageNet과 같은 대형 데이터셋에서 훈련된 모델을 다른 작업에 활용할 수 있습니다.
    - **예시**: ResNet, BERT와 같은 모델들은 다양한 작업에 대해 사전 훈련되어 있고, 이를 다른 특정 작업에 맞게 **파인튜닝(fine-tuning)**하여 사용할 수 있습니다.
- 정규화 하는 이유 - 일관성을 유지하기 위합입니다. 
    # 데이터 정규화
    train_images = train_images / 255.0
    test_images = test_images / 255.0 - 픽셀값을 정규화 하가 
- 미세 조정 - 파인 튜닝
    미세 조정하는 이유는 사전학습된 모델을 새로운 데이터셋에 맞추기 위해 추가로 학습시키는 과정입니다. 
    이를 통해 모델의 성능을 향상시킬 수 있으며, 다음과 같은 이유로 미세 조정을 수행  
- 부분적인 동결(FREEZE), 전체 모델 학습, 새로운 출력층 증가 → TRANSFER LEARNING
- 더 가까운 값으로 만든다.
- 미세 조정했을 때 가 사전학습할 때보다 정확도가 좋아야 함
- **장점**:
    - 학습 시간 단축
    - 성능 향상
    - 작은 데이터셋에 효과적
    - ex) 모델의 input shape을 맞춰야 함
```python
python
코드 복사
# 예시 코드
from tensorflow.keras.applications import VGG16

# VGG16 사전 학습 모델 가져오기 (include_top=False는 출력층 제외)
vgg_base = VGG16(weights='imagenet', include_top=False)

# 일부 가중치 학습 해제하고 미세 조정
vgg_base.trainable = True
```

---
### **8. Gradient Exploding (그레이디언트 폭발)**
그레이디언트 폭발은 신경망을 학습할 때 그레이디언트 값이 점점 커져서, 가중치 갱신이 비정상적으로 커지며 학습이 불안정해지는 현상임. 
이 문제는 특히 RNN처럼 반복 구조를 가진 네트워크에서 자주 발생하는데, 가중치가 너무 커지면 모델이 발산해서 학습이 안 됨. 
이걸 해결하기 위해서는 **gradient clipping**을 사용해서 그레이디언트 값이 일정 크기를 넘지 않도록 제한할 수 있음.
- **예시**: LSTM에서 ReLU를 쓰면 폭발 가능성이 있어서 보통 tanh를 씀.

---
### **10. Embedding (임베딩)**
임베딩은 주로 텍스트 데이터를 숫자 벡터로 변환해 머신러닝 모델이 이해할 수 있게 만드는 과정임. 
`label encoding`이나 `one-hot encoding`과 다르게 **text embedding**은 단어 간의 의미와 관계를 더 잘 보존하며, 
딥러닝에서는 보통 학습을 통해 유사한 단어를 가까운 위치에 배치함. 이때 **유클리드 거리**로 단어 간 유사성을 측정함.

---
### **11. Variant CNN (변형된 CNN 모델들)**
CNN은 다양한 변형 모델들이 있는데, 각각의 변형은 특정 성능 향상을 위해 고안된 특성을 가짐:
- **LeNet-5**: 초기 모델로, 모든 활성화 함수로 tanh를 사용함.
- **AlexNet**: ReLU 활성화와 softmax를 도입함으로써 학습 속도와 성능을 높임.
- **GoogLeNet (Inception 모델)**: 여러 크기의 커널을 동시에 사용해 다양한 크기의 특성 맵을 추출하며, 
1x1 합성곱 층을 통해 차원을 줄여 연산 효율을 높이고 일반화 성능을 향상시킴.
- **Xception :** Xception은 **GoogLeNet**에 기반하면서도 성능을 더 높이기 위해 **깊이별 분리 합성곱층(depthwise separable convolution)**을 도입한 모델임. 
이 방식은 연산을 두 단계로 나눠서 처리하는데, 일반 합성곱보다 계산 효율이 좋고 복잡한 패턴을 효과적으로 감지함.
- **깊이별 분리 합성곱의 두 단계**:
    1. **Depthwise Convolution**: 각 채널에 대해 독립적으로 합성곱을 수행하여 공간적 패턴을 감지함.
    2. **Pointwise Convolution (1x1 Convolution)**: 1x1 합성곱으로 채널 간 상호작용을 학습함.
- 이 구조 덕분에 모델은 더 적은 계산량으로 높은 성능을 유지할 수 있음.
  특히 **ResNet의 잔차 연결**(residual connections)과 **인셉션 모듈의 특징**을 결합하여 성능과 효율성을 극대화함. 
- **Inception-v4**도 이 방식을 사용해 연산량을 줄이고, 모델 복잡도와 효율성을 동시에 높임.
- **SENet (Squeeze-and-Excitation Network) :** SENet은 신경망의 특정 특징 맵에 가중치를 조정하여, 중요도가 높은 특징에 집중하도록 설계된 모델임. 
  **Squeeze 단계**에서는 글로벌 평균 풀링으로 각 채널의 전역 정보를 압축하고, **Excitation 단계**에서는 이 정보를 이용해 채널별 가중치를 조정함. 
  이 과정을 통해 중요한 채널은 강화하고 덜 중요한 채널은 약화시켜, 네트워크가 더 유의미한 특징을 학습할 수 있게 함. 
  이는 이미지 분류 등 다양한 작업에서 성능을 크게 향상시킴.
- **DenseNet :**  레이어들이 서로 **집약적으로 연결**되어, 이전 레이어의 출력이 모든 이후 레이어의 입력으로 전달되는 구조임. 
  각 레이어가 입력 정보와 그레이디언트를 재사용할 수 있어, **파라미터 효율성**이 높고 **학습이 빨리 수렴**함. 
  이로 인해 상대적으로 **작은 모델 크기로도 높은 성능**을 낼 수 있음.

---
### **12. Random Forest와 Regression에서의 Feature Correlation**
1. **Random Forest**: 트리 기반 모델에서는 비슷한 feature들을 동시에 포함해도 좋은 결과를 낼 수 있음. 
  이는 모델이 다양한 특성을 랜덤하게 선택해 여러 트리를 학습하기 때문에, feature가 다소 비슷해도 상관없음. 
  즉, **차원의 저주(curse of dimensionality)**에 덜 영향을 받음.
2. **Regression**: 반면, 회귀 모델에서는 비슷한 feature들이 중복되는 경우, 한쪽 feature만 사용하는 것이 낫다.
  비슷한 feature들을 동시에 포함하면 과적합의 위험이 높아지고, 회귀 계수가 불안정해질 수 있음.
