week11

향후 5년 ai 없이 아무것도 할 수 없다. 
분야를 얼른 정해라 

전체적 교재 review
- 이거 위주로 공부할 것 

- train과 test로 나누는 이유
    - 모델의 일반화 능력을 확인하기 위해서
    - 교차 검증 하는 이유는 모델이 특정 데이터셋에 맞을 가능성이 있기 때문에 k개로 나눠서 교차 검증한다.
- 비지도 학습의 방법
    - self - supervisied learning  비지도 학습의 일부
    - semi- supervisied learning
- feature selection  성능을 높이기 위해서 불필요한 특성을 제거하기 위해서
- overfitting 해결법
    - 모델을 단순화  regularization 규제 l1 norm, l2 norm
    - dropout
    - early start fitting
- under fitting 해결법
    - 가장 큰 문제  데이터 부족
    - 데이터 증가해야 함 arguementation → 다양성이 많으면서 smotz
    - 이미지는 좀 낫다 rotation, shift가 가능, 임상학적으로 느낌이 다른가 그건 불가능하다
- embedding
    - 범주형 데이터 같은 고차원 데이터를 벡터 형태의 저차원 공간에 표현하는 기법
    - 임베딩 하는 이유  차원 축소
    - 고차원 데이터를 저차원 벡터로 변환해 의미와 관계를 보존하면서도 모델이 효율적으로 처리할 수 있도록 돕는 핵심 기법
- one hot encoding
    - 범주형 데이터를 이진 터로 변환하는 가장 단순한 방법임. 범주의 개수만큼 벡터의 길이를 설정하고, 해당 범주에 해당하는 위치를 `1`, 나머지는 `0`으로 표시함.
- vector encoding
    - 임베딩(embedding) 기법을 사용하여, 각 범주를 의미 있는 저차원 벡터로 표현함. 단어 간 유사도나 관계를 벡터 내에서 반영할 수 있는 장점이 있음. 임베딩을 통해 각 범주가 고정된 길이의 숫자 벡터로 변환되며, 이 벡터는 모델이 학습을 통해 조정할 수 있음. 벡터는 실수 값으로 구성되어 있음
- scaling
    - 스케일링은 데이터를 일정한 범위로 변환하여 모델 학습을 더 효과적으로 하는 방법임. 특히 거리 기반 알고리즘이나 신경망에서 중요하게 사용됨. 주로 표준화(Standardization)와 정규화(Normalization) 방식이 있음
    - 학습 안정화 특성(feature)들의 크기가 너무 다르면, 일부 큰 값의 특성이 모델을 주로 지배하게 되어 학습이 잘 안됨. 이를 방지하고 안정적인 학습이 가능해짐.
    - 속도 향상 데이터가 균일한 범위에 있으면 학습이 빨라짐.
    - 성능 향상 거리 기반 모델(KNN, SVM 등)에서 특성 간 스케일 차이를 줄여 성능이 개선됨.
- k-fold 교차 검증
    - 데이터를 나눠서 다양한 경우에 대해 성능을 평가하여 일반화 능력을 확인하기 위해 사용
- 앙상블
    - 여러 모델을 결합하여 예측 성능을 향상하는 방법임. 개별 모델들의 약점을 보완하고 강점을 살려 더 안정적이고 높은 성능의 모델을 만드는 기법임. 대표적인 앙상블 기법으로는 배깅(Bagging), 부스팅(Boosting), 스태킹(Stacking) 등이 있음.
- random forest
    - 기본적으로 샘플을 boot strapping 한다. 중복을 허락bagging 해서 여러 개의 샘플을 만든다. 반) 중복을 허락하지 않는 것은 뭔지 찾아봐라 페이스팅
- hyper parameter tuning
    - 모델의 성능을 높이기 위해서
    - 모델이 데이터를 얼마나 잘 이해하고 일반화할 수 있을지 결정하는 중요한 변수들이 하이퍼 파라미터에 의해 설정됨 , 모델이 맞는 최적의 값을 찾아주는 것
    - 하이퍼 파라미터  모델 학습 과정에서 조정되지 않고 사람이 미리 설정해야 하는 값
- confusion_matrix ()
    - 오차 행렬을 줄이기 위해서 사용
    - 어느 class에서 분류가 잘 안 됐는지 알고 싶어서 다중class의 경우에는 class 간의 분류를 확인 하기 위해서
    - accuracy 대신 f1 값을 쓰는 이유 → 데이터 불균형
        
        ![image.png](httpsprod-files-secure.s3.us-west-2.amazonaws.com098b8f36-c66d-4641-adf9-693ce54b341524766e08-d553-44a2-902b-82cf7a05f9e3image.png)
        
- f1 점수
- 경사하강법
    - 손실함수의 미분값(기울기)를 최소화 하는 최적화 알고리즘
    - 예측값과 손실함수를 최소화 하는 것, 오차를 줄이는 것
    - 배치 경사하강법 모든 훈련 데이터를 사용하여 기울기를 계산하고 파라미터를 업데이트함. 너무 오래 걸림
    - 확률적 경사하강법 한 개의 샘플을 사용해 기울기 계산 그에 따라 파라미터 업데이트
    - 미니 배치 경사하강법 훈련 데이터를 작은 배치로 나누어 각 배치에서 기울기를 계산하고 파라미터 업데이트
    - 단점
- rsmprop, adam → 오차를 줄이는 함수
- 규제 rinch, lasso, 조기 종료
- 선형 SVM - 서포트 벡터 머신
    - 서포트 벡터  결정 경계  근처에 있는  값 그것의 마진이 커지면 분류가 잘 된다.
- 지니 불순도
    - 결정 트리에서 노드를 분할할 때 기준으로 사용하는 불순도 지표임. 특정 노드에 포함된 데이터들이 얼마나 섞여 있는지를 나타냄.
- 차원의 저주
    - 특징이 지나치게 많으면 모델이 패턴을 파악하기 어려운 것
- pca
    - 차원을 압축하는 것
- 매니폴드 학습
    - 매니폴드 가설  고차원의 데이터들이 사실상 저차원의 구조(매니폴드)에 놓여 있다는 가설임. 고차원의 데이터들이 공간 전체에 고르게 퍼져 있는 것이 아니라, 저차원 매니폴드 상에 위치해 있다는 의미임.
    - 예를 들어, 사람 얼굴 사진을 찍으면 사진 데이터는 수백, 수천 개의 픽셀 값으로 이루어져 있으므로 고차원임. 그런데 실제로는 사람의 얼굴을 결정짓는 특징(예 코 위치, 눈의 크기 등)은 몇 가지 주요 요소로 표현할 수 있음.
        - 따라서, 실제로 중요한 정보는 고차원 데이터의 특정 저차원 공간에 집중되어 있다는 것
    - 매니폴드 가설은 데이터의 본질적인 구조를 파악해 차원 축소, 효율적 학습, 그리고 새로운 데이터 생성 등에 활용할 수 있게 도와주는 개념
- activation function 쓰는 이유   각 층마다 비선형성 결과값을 부여하면서 더 복잡한 특징을 추출할 수 있기 때문에 활성화 함수를 쓴다.  그래디언트 배니싱 현상이 일어난다 그래서 relu 사용
- 그래디언드 폭주와 소실 ( 자주 얘기해서 넘어감)
- pretrained model
    - 근삿값을 가지고 미세조정하는 모델
- auto encoder
- cnn
    - 기본적인 차이점  인접한 값의 특정 추출할 수 있다.
    - 풀링   정보 추출, 불변성
    - resnet
        - 잔차학습  입력과 거쳐온 차이를 비교하면서 학습을 용이하게 만드는 것
    - 인셉션
    - senet  어느 채널에 집중할 것인가 채널별 중요도를 먹이는 것
    - varient cnn의 궁극적인 목적  경량화 light weight
    - 어떻게 경량화 하면서 기울기 소실 문제를 해결할까 중요한 채널들에 집중하자 (senet) depthwise 뭐드라 dwcnn  pare wise (xeption) 결국 어떤 것에 attention 할지 집중해야 한당(git 참고)
- RNN
- attention
    - scaledot attention
        - 전체 크기를 맞춰주는 것
        - masked 시험에 안 나옴 nlp( 자연어 처리 , 언어들을 어떻게 자연어 처리 할 것인가)
- transformer
    - 딥러닝에서 주로 자연어 처리(NLP)에 사용되던 모델임. 하지만, 그 구조가 확장성과 병렬 처리에 유리해 다양한 영역에서 각광받고 있음. 기본적으로 어텐션 메커니즘을 사용해 데이터 내의 중요한 정보에 집중하도록 설계되어 있음.
    - 셀프 어텐션(Self-Attention) 각 단어(혹은 토큰)가 문장의 다른 모든 단어와의 관계를 학습함. 이를 통해 문맥을 잘 이해하고, 장기 의존성을 잡아낼 수 있음.
    - 병렬화 가능 RNN과 달리 모든 토큰을 동시에 처리 가능해서, 병렬 연산이 가능하고 훈련 속도가 빠름.
    - 스케일 가능성 대규모 데이터와 파라미터 수를 가진 모델로 확장할 수 있음 (GPT, BERT, T5, BERT 같은 모델들이 트랜스포머 구조를 기반으로 함).
- 비전 트랜스포머
    - 이미지 분할 이미지를 작은 패치(patch)로 분할하여 각각을 일종의 토큰으로 간주함.
    - 토큰 임베딩 분할된 패치를 벡터로 변환하여 트랜스포머의 입력으로 넣음. 이 벡터는 패치의 정보를 포함한 일종의 임베딩(embedding)임.
    - 포지셔널 임베딩(Positional Embedding) 트랜스포머에는 순서 개념이 없으므로, 패치들이 이미지에서 어디 위치에 있는지 정보도 추가로 입력됨.
    - 셀프 어텐션 적용 패치 간의 관계를 어텐션을 통해 학습함. 이 과정에서 비전 트랜스포머는 이미지 내의 다양한 지역적, 전역적 특징을 파악함.
    - 장점 CNN처럼 지역적 특징에만 국한되지 않고, 이미지 전체의 글로벌 특징을 더 잘 학습할 수 있음.
    - 한계 패치 기반 처리 방식은 많은 데이터와 연산 능력이 필요함. 따라서 대규모 데이터가 없을 경우 CNN보다 성능이 낮을 수 있음.
    
    비전 트랜스포머는 이미지 데이터를 시퀀스 데이터처럼 다루어 트랜스포머 구조의 강점을 비전 분야에서도 발휘할 수 있도록 고안된 모델임.
    
- ae, gan (10주차 요약 보기)

시험 
10시 1시간 415호
100프로 이론 시험 
- 시험 문제 몇 문제
- o,x 달달 외우기 x 이게 맞는지 틀리는지  그냥 내 걸로 만들어라
- 문제 서술형 디테일하게 서술해야 한다.